Scope:
- Target language: Java - Most benchmark datasets for auto-generated test code are Java.

- Scripting lang: Python - working with LLM is way easier in python, and simple fast prototyping

Tasks:
1. Define pipeline - LLM, clustering and other needed things
2. A robust way of analysing and defining metrics 

Potential pipeline:
Using multiple methods instead of a single LLM, to improve efficiency

1. Using semantic similarity or context aware algo to make embeddings

2. using clustering's to combine similar items

3. LLM 
	- test multiple prompts engineering 

4. multiple benchmarks
	- test with or without multiple parts to understand the impact
	- keep metrics of power and other consumption of each part
	- measure statistics compared to previous thesis

5. add human intervention possibility (optional)

Potential research questions:
- To what extent can large language models (LLMs) generate meaningful variable names for automatically generated test code with obfuscated or meaningless identifiers?

- How does the performance of LLM-based variable name generation compare to existing machine learning models such as RefBERT or GGNN? ( comparing to previous thesis )

- How do different prompt engineering strategies affect the quality and consistency of variable name suggestions generated by LLMs?

- How do developers perceive the readability, intent clarity, and maintainability of test code with LLM-generated variable names compared to traditional approaches?

- Do LLM-generated variable names maintain consistent semantics across similar code scenarios within and across projects?

- How does fine-tuning an LLM on test-specific code (e.g., unit tests from GitHub) affect naming quality compared to using a general-purpose code LLM?