# Dataset directories
TRAIN_DIR: train
VAL_DIR: val
TEST_DIR: test

OUTPUT_DIR: /out/data_preprocessed/
INPUT_DIR: in

# Model save path
SAVE_MODEL_PATH: /out/model/
ADAPTER_SAVE_PATH: /out/adapter/

# Logs
LOG_DIR: /out/logs

# Model parameters
MODEL_ID: Qwen/Qwen2.5-Coder-7B-Instruct

MAX_LENGTH: 4096
BATCH_SIZE_PER_DEVICE: 1
GRAD_ACCUM_STEPS: 8
NUM_EPOCHS: 3
LEARNING_RATE: 0.0001
MAX_SAVE_TOTAL: 3
LOGGING_STEPS: 20
EVAL_STEPS: 200
SAVE_STEPS: 200
SAVE_TOTAL_LIMIT: 3

# QLoRA parameters (https://datawizz.ai/blog/understanding-lora-adapters-rank-and-alpha-parameters)
USE_QLORA: True # If not using QLoRA, we will do regular fine-tuning LoRA.
RANK: 16 # The rank determines the dimensionality of the low-rank decomposition. High R -> higher adaptation capacity but more memory and compute cost.
LORA_ALPHA: 32 # is a scaling factor that controls the contribution of the LoRA adaptation to the original weights.
LORA_DROPOUT: 0.05 # in LoRA randomly zeros out some of the activations before the low-rank adapters during training, just like regular dropout.
