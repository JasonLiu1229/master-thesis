services:
  t1:
    container_name: t1
    build:
      dockerfile: docker/t1.Dockerfile
    volumes:
      - ./out:/app/code/out/
    working_dir: /app/code
    gpus: all
    profiles: ["t1", "arno", "test"]
    networks:
      - codereader

  t2:
    container_name: t2
    build:
      dockerfile: docker/t2.Dockerfile
    volumes:
      - ./out:/app/code/out/
    working_dir: /app/code
    gpus: all
    profiles: ["t2", "arno", "test"]
    networks:
      - codereader

  t3:
    container_name: t3
    build:
      dockerfile: docker/t3.Dockerfile
    volumes:
      - ./out:/app/out/
    env_file:
      - .env
    profiles: ["t3", "test", "full_local"]
    depends_on:
      api:
        condition: service_healthy
    networks:
      - pipeline

  t3_eval:
    container_name: t3_eval
    build:
      dockerfile: docker/t3_eval.Dockerfile
    volumes:
      - ./out:/app/out/
    env_file:
      - .env
    profiles: ["t3_eval", "full_local_eval"]
    depends_on:
      api:
        condition: service_healthy
      codereader_ollama:
        condition: service_healthy
    environment:
      CODEREADER_URL: http://codereader_ollama:8080
      OLLAMA_HOST: http://codereader_ollama:11434
    networks:
      - pipeline
      - codereader

  t3_eval_openai:
    container_name: t3_eval_openai
    build:
      dockerfile: docker/t3_eval.Dockerfile
    volumes:
      - ./out:/app/out/
    env_file:
      - .env
    profiles: ["t3_eval_openai"]
    depends_on:
      codereader_ollama:
        condition: service_healthy
    environment:
      CODEREADER_URL: http://codereader_ollama:8080
      OLLAMA_HOST: http://codereader_ollama:11434
    networks:
      - pipeline
      - codereader

  benchmark:
    container_name: benchmark
    build:
      dockerfile: docker/benchmark.Dockerfile
    volumes:
      - ./out:/app/code/out/
    working_dir: /app/code
    depends_on:
      t1:
        condition: service_completed_successfully
      t2:
        condition: service_completed_successfully
    gpus: all
    profiles: ["benchmark", "arno"]

  api:
    container_name: api
    build:
      dockerfile: docker/api.Dockerfile
    ports:
      - "8000:8000"
    gpus: all
    env_file:
      - .env
    profiles: ["api", "full_local", "full_local_eval"]
    volumes:
      - hf_cache:/root/.cache/huggingface/
      - model_cache:/app/model/local_model/
      - ./out:/app/out/
    networks:
      - pipeline
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

  tuner:
    container_name: tuner
    build:
      dockerfile: docker/tuner.Dockerfile
    volumes:
      - ./out:/app/out/
      - hf_cache:/root/.cache/huggingface/
    working_dir: /app
    gpus: all
    profiles: ["tune"]
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc: 16384
      stack: 67108864
      memlock:
        soft: -1
        hard: -1
    ipc: host

  tuner_preprocess:
    container_name: tuner_preprocess
    build:
      dockerfile: docker/tuner_preprocess.Dockerfile
    volumes:
      - ./out:/app/out/
      - hf_cache:/root/.cache/huggingface/
    working_dir: /app
    gpus: all
    profiles: ["tune_process"]
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc: 16384
      stack: 67108864
      memlock:
        soft: -1
        hard: -1
    ipc: host

  codereader_ollama:
    container_name: codereader_ollama
    build:
      dockerfile: docker/codereader.Dockerfile
    gpus: all
    profiles: ["full_local_eval", "t3_eval", "codereader"]
    networks:
      - codereader
    ports:
      - "11434:11434"
      - "8080:8080"
    volumes:
      - ollama_cache:/root/.ollama
      - ./out:/app/out/
    environment:
      CODEREADER_CONFIG_FILE: /app/codereader.yml
      OLLAMA_HOST: http://127.0.0.1:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 5m
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

volumes:
  hf_cache:
  model_cache:
  ollama_cache:

networks:
  pipeline:
    driver: bridge
  codereader:
    driver: bridge
